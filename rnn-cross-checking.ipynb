{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-23T10:12:29.494209Z",
     "iopub.status.busy": "2024-11-23T10:12:29.493112Z",
     "iopub.status.idle": "2024-11-23T10:12:47.351971Z",
     "shell.execute_reply": "2024-11-23T10:12:47.350786Z",
     "shell.execute_reply.started": "2024-11-23T10:12:29.494153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, GRU, SimpleRNN\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T10:20:55.355648Z",
     "iopub.status.busy": "2024-11-23T10:20:55.355227Z",
     "iopub.status.idle": "2024-11-23T10:20:56.919684Z",
     "shell.execute_reply": "2024-11-23T10:20:56.918201Z",
     "shell.execute_reply.started": "2024-11-23T10:20:55.355601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Wczytanie embeddignów wygenerowanych przez trzy modele\n",
    "\n",
    "names_of_models = ['LSTM', 'GRU', 'RNN']\n",
    "\n",
    "all_person_embedding = []\n",
    "\n",
    "for name in names_of_models:\n",
    "    \n",
    "    file_path = '/kaggle/input/rnn-test-embedding/all_person_embedding_'\n",
    "\n",
    "    embedding_one_model = []\n",
    "    \n",
    "    for i in range(0,5):\n",
    "        one_dataset = file_path + str(i) + '_' + name + '.pkl'\n",
    "\n",
    "        with open(one_dataset, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "\n",
    "        embedding_one_model.extend(data)\n",
    "        \n",
    "    all_person_embedding.append(embedding_one_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T10:34:22.655006Z",
     "iopub.status.busy": "2024-11-23T10:34:22.654631Z",
     "iopub.status.idle": "2024-11-23T10:34:23.681035Z",
     "shell.execute_reply": "2024-11-23T10:34:23.679983Z",
     "shell.execute_reply.started": "2024-11-23T10:34:22.654975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for iterator_for_model in range(0, len(all_person_embedding)):\n",
    "    \n",
    "    embedding_all_people = all_person_embedding[iterator_for_model]\n",
    "    \n",
    "    #Embedding enrollment ma być stworzony z 40 s nagrania\n",
    "    emrollment_embedding_all_people = []  # Lista do przechowywania embeddingów enrollment dla wszystkich osób\n",
    "    test_embedding_all_people = []  # Lista do przechowywania embeddingów testowych dla wszystkich osób\n",
    "    \n",
    "    # Pętla przetwarzająca embeddingi dla każdej osoby w embedding_all_people\n",
    "    for i in range(0, len(embedding_all_people)):\n",
    "        \n",
    "        # Inicjalizacja zerowych embeddingów dla enrollment i testowych (o długości 128)\n",
    "        enrollment_embedding = np.zeros(128)\n",
    "        test_embedding = np.zeros(128)\n",
    "        test_embedding_one_person = []  # Lista do przechowywania testowych embeddingów dla jednej osoby\n",
    "    \n",
    "        # Zaokrąglenie liczby fragmentów danej osoby do najbliższej wielokrotności 10, aby nagrania testowe miały długość 10 s.\n",
    "        rounded_len = np.floor_divide(len(embedding_all_people[i]), 10) * 10\n",
    "        \n",
    "        for j in range(0, rounded_len):\n",
    "            \n",
    "            # Dodawanie embeddingów do enrollment dla pierwszych 40 fragmentów - enrollment ma być zbudowany z \n",
    "            # 40 s nagrania\n",
    "            if j < 40:\n",
    "                enrollment_embedding += embedding_all_people[i][j][0]\n",
    "            \n",
    "            # Po co dziesiątym fragmencie: obliczenie średniego embeddingu testowego i resetowanie `test_embedding`\n",
    "            # embedding test ma mieć długośc 10 s\n",
    "            elif (j + 1) % 10 == 0:\n",
    "                test_embedding = test_embedding / 10\n",
    "                test_embedding_one_person.append(test_embedding)  # Dodanie średniego embeddingu do listy\n",
    "                test_embedding = np.zeros(128)  # Reset embeddingu testowego do kolejnych obliczeń\n",
    "            \n",
    "            # Dodawanie embeddingów do testowego dla kolejnych fragmentów\n",
    "            else:\n",
    "                test_embedding += embedding_all_people[i][j][0]\n",
    "        \n",
    "        # Obliczenie średniego embeddingu enrollmentowego (średnia z 40 fragmentów)\n",
    "        enrollment_embedding = enrollment_embedding / 40\n",
    "    \n",
    "        # Dodanie średniego embeddingu enrollment i listy testowych embeddingów do głównych list\n",
    "        emrollment_embedding_all_people.append(enrollment_embedding)\n",
    "        test_embedding_all_people.append(test_embedding_one_person)\n",
    "    \n",
    "        # Wyświetlanie postępu przetwarzania dla każdej osoby\n",
    "        print((i + 1) / len(embedding_all_people))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    \n",
    "    with open('enroll_' + names_of_models[iterator_for_model] + '.pkl', 'wb') as file:\n",
    "        pickle.dump(emrollment_embedding_all_people, file)\n",
    "\n",
    "    with open('test_' + names_of_models[iterator_for_model] + '.pkl', 'wb') as file:\n",
    "        pickle.dump(test_embedding_all_people, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T10:42:48.934298Z",
     "iopub.status.busy": "2024-11-23T10:42:48.933513Z",
     "iopub.status.idle": "2024-11-23T10:42:49.007744Z",
     "shell.execute_reply": "2024-11-23T10:42:49.006645Z",
     "shell.execute_reply.started": "2024-11-23T10:42:48.934260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Wczytuję wyżej policzone embedding enrollment oraz test\n",
    "\n",
    "enrolls = []\n",
    "tests = []\n",
    "\n",
    "enroll_test_list = ['enroll', 'test']\n",
    "\n",
    "for witch_data in enroll_test_list:\n",
    "    \n",
    "    path = '/kaggle/working/' + witch_data + '_'\n",
    "    \n",
    "    for name in names_of_models:\n",
    "        \n",
    "        one_dataset = path + name + '.pkl'\n",
    "        \n",
    "        with open(one_dataset, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "\n",
    "        if witch_data == 'enroll':\n",
    "            enrolls.append(data)\n",
    "        else:\n",
    "            tests.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T10:46:04.160586Z",
     "iopub.status.busy": "2024-11-23T10:46:04.160133Z",
     "iopub.status.idle": "2024-11-23T10:48:09.493961Z",
     "shell.execute_reply": "2024-11-23T10:48:09.492870Z",
     "shell.execute_reply.started": "2024-11-23T10:46:04.160547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ID: 49\n"
     ]
    }
   ],
   "source": [
    "# W pętli liczone są podobieństwa kosinusowe pomiędzy enrollment a test dla embeddingów wgenerowanych przez każdy model, \n",
    "# na podstawie tych danych będą przeprowadzone ewaluacjie modeli\n",
    "\n",
    "for iterator_for_model in range(0, len(names_of_models)):\n",
    "    # Iteracyjnie pobierane są kolejne dane odpowiadające kolejnym modelom\n",
    "    emrollment_embedding_all_people = enrolls[iterator_for_model]\n",
    "    test_embedding_all_people = tests[iterator_for_model]\n",
    "    \n",
    "    # DataFrame do przechowywania wyników\n",
    "    data_rows = []\n",
    "    data_rows_trans = []\n",
    "    \n",
    "    for i in range(0, len(emrollment_embedding_all_people)):\n",
    "        embedding_enrollment = emrollment_embedding_all_people[i].reshape(1, -1)\n",
    "    \n",
    "        for j in range(0, len(test_embedding_all_people)):\n",
    "            # Wczytanie embeddingów testowych dla danej osoby\n",
    "            one_person_list = np.array(test_embedding_all_people[j])\n",
    "            random_numbers = np.random.uniform(0, 1, len(one_person_list)) > 0.5\n",
    "            one_person_list = one_person_list[random_numbers]\n",
    "    \n",
    "            for h in range(0, len(one_person_list)):\n",
    "                # Wczytanie embeddingu testowego\n",
    "                embedding_eval = one_person_list[h].reshape(1, -1)\n",
    "                # Obliczenie podobieństwa kosinusowego\n",
    "                cos_sim = cosine_similarity(embedding_enrollment, embedding_eval)[0][0]\n",
    "                # Zapisanie wyników\n",
    "                data_rows.append([i, j, i == j, cos_sim])\n",
    "    \n",
    "        clear_output(wait=True)\n",
    "        print(f\"Processing ID: {i}\")\n",
    "    \n",
    "    # Tworzenie DataFrame z listy wyników\n",
    "    long_data_frame = pd.DataFrame(data_rows, columns=['ID_enrollment', 'ID_test', 'genuine', 'score'])\n",
    "    long_data_frame.to_csv('long_data_frame' + names_of_models[iterator_for_model] + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6138908,
     "sourceId": 9977261,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6138458,
     "sourceId": 9990049,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
